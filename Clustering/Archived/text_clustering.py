# -*- coding: utf-8 -*-
"""Text_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xpEQu4VI1k8I6iYTE9vV2jJHBsWkWUbR
"""

# imports / libraries used
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import euclidean_distances

# load data
df = pd.read_csv('Text_Data.csv')

# keep columns we need - vibe, activities
text_data = df.iloc[:, 3:]

# generate embeddings for the data with Sentence-Bert since we have full sentences and need context
model = SentenceTransformer('all-MiniLM-L6-v2')

text_embeddings = {}

for col in text_data.columns:
    col_texts = text_data[col].astype(str).tolist()
    text_embeddings[col] = model.encode(col_texts)
    print(f"Generated embeddings for '{col}', shape: {text_embeddings[col].shape}")

# combine embeddings from both columns so both can be used in clustering
all_embeddings = np.hstack([text_embeddings['Vibe'], text_embeddings['Activities']])
print(f"Combined embeddings shape: {all_embeddings.shape}")

# standardize the embeddings
scaler = StandardScaler()
scaled_embeddings = scaler.fit_transform(all_embeddings)

# perform k-means clustering - evaluate several k values
wcss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_embeddings)
    wcss.append(kmeans.inertia_)

# review elbow graph to determine optimal k
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o')
plt.title("Elbow Method")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.show()

# run k-means with optimal k and save clusters
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(scaled_embeddings)
df["Cluster"] = clusters

# determine which neighborhoods are closest together (without cluster info)

def find_closest_neighborhood(city1, neighborhood1, city2, df):
    # normalize city and neighborhood data
    city1 = city1.strip().lower()
    neighborhood1 = neighborhood1.strip().lower()
    city2 = city2.strip().lower()

    df["City_normalized"] = df["City"].str.strip().str.lower()
    df["Neighborhood_normalized"] = df["Neighborhood"].str.strip().str.lower()

    ### filter what data we are actually looking at (based on user input) and clean (NaN --> 0)

    # filter city 1 data
    city1_data = df[df["City_normalized"] == city1]
    if city1_data.empty:
        raise ValueError(f"No data found for city '{city1}'.")

    # set NaN values to 0
    city1_data = city1_data.fillna(0)

    # filter city  data
    city2_data = df[df["City_normalized"] == city2]
    if city2_data.empty:
        raise ValueError(f"No data found for city '{city2}'.")

    # set NaN values to 0
    city2_data = city2_data.fillna(0)

    ### get data for neighborhood1
    neighborhood1_data = city1_data[city1_data["Neighborhood_normalized"] == neighborhood1]
    if neighborhood1_data.empty:
        raise ValueError(f"No data found for neighborhood '{neighborhood1}' in city '{city1}'.")

    # extract the vibe and activities columns for embedding for neighborhood1
    neighborhood1_vibe = neighborhood1_data["Vibe"].values[0]
    neighborhood1_activities = neighborhood1_data["Activities"].values[0]

    # perform embedding for neighboorhood1
    model = SentenceTransformer('all-MiniLM-L6-v2')
    neighborhood1_vibe_embedding = model.encode([neighborhood1_vibe])[0]
    neighborhood1_activities_embedding = model.encode([neighborhood1_activities])[0]

    # combine embeddings for neighboorhood1
    neighborhood1_combined_embedding = np.concatenate([neighborhood1_vibe_embedding, neighborhood1_activities_embedding])

    # extract the vibe and activities columns for embedding for city2
    city2_vibes = city2_data["Vibe"].tolist()
    city2_activities = city2_data["Activities"].tolist()

    # perform embedding for city2
    city2_vibe_embeddings = model.encode(city2_vibes)
    city2_activities_embeddings = model.encode(city2_activities)

    # combine embeddings for city2
    city2_combined_embeddings = np.concatenate([city2_vibe_embeddings, city2_activities_embeddings], axis=1)

    # compute distances between the neighborhood1 embeddings and city2 embeddings
    distances = euclidean_distances([neighborhood1_combined_embedding], city2_combined_embeddings)

    # find and return index of closest neighborhood in city2
    closest_idx = np.argmin(distances)
    return city2_data.iloc[closest_idx]["Neighborhood"]

### Testing
closest_no_cluster = find_closest_neighborhood("New York City", "Bushwick and Williamsburg", "San Francisco", df)
print("Closest neighborhood:", closest_no_cluster)

# determine which neighborhoods are closest together within a cluster

def find_closest_neighborhood_cluster(city1, neighborhood1, city2, df, optimal_k):
    # normalize city and neighborhood data
    city1 = city1.strip().lower()
    neighborhood1 = neighborhood1.strip().lower()
    city2 = city2.strip().lower()

    df["City_normalized"] = df["City"].str.strip().str.lower()
    df["Neighborhood_normalized"] = df["Neighborhood"].str.strip().str.lower()

    ### filter what data we are actually looking at (based on user input) and clean (NaN --> 0)

    # filter city 1 data
    city1_data = df[df["City_normalized"] == city1]
    if city1_data.empty:
        raise ValueError(f"No data found for city '{city1}'.")

    # set NaN values to 0
    city1_data = city1_data.fillna(0)

    # filter city 2 data
    city2_data = df[df["City_normalized"] == city2]
    if city2_data.empty:
        raise ValueError(f"No data found for city '{city2}'.")

    # set NaN values to 0
    city2_data = city2_data.fillna(0)

    ### get data for neighborhood1
    neighborhood1_data = city1_data[city1_data["Neighborhood_normalized"] == neighborhood1].iloc[:, 4:]
    if neighborhood1_data.empty:
        raise ValueError(f"No data found for neighborhood '{neighborhood1}' in city '{city1}'.")

    # get cluster labels for neighborhood1
    neighborhood1_cluster = city1_data[city1_data["Neighborhood_normalized"] == neighborhood1]["Cluster"].iloc[0]

    # extract the vibe and activities columns for embedding for neighborhood1
    neighborhood1_vibe = city1_data[city1_data["Neighborhood_normalized"] == neighborhood1]["Vibe"].values[0]
    neighborhood1_activities = city1_data[city1_data["Neighborhood_normalized"] == neighborhood1]["Activities"].values[0]

    # perform embedding for neighboorhood1
    model = SentenceTransformer('all-MiniLM-L6-v2')
    neighborhood1_vibe_embedding = model.encode([neighborhood1_vibe])[0]
    neighborhood1_activities_embedding = model.encode([neighborhood1_activities])[0]

    # combine embeddings for neighboorhood1
    neighborhood1_combined_embedding = np.concatenate([neighborhood1_vibe_embedding, neighborhood1_activities_embedding])

    # extract data from city2 that is in neighborhood1 cluster
    city2_in_same_cluster = city2_data[city2_data["Cluster"] == neighborhood1_cluster]

    if not city2_in_same_cluster.empty:

        # extract the vibe and activities columns for embedding for city2
        city2_vibes = city2_in_same_cluster["Vibe"].tolist()
        city2_activities = city2_in_same_cluster["Activities"].tolist()

        # perform embedding for city2
        city2_vibe_embeddings = model.encode(city2_vibes)
        city2_activities_embeddings = model.encode(city2_activities)

        # combine embeddings for city2
        city2_combined_embeddings = np.concatenate([city2_vibe_embeddings, city2_activities_embeddings], axis=1)

        # compute distances between the neighborhood1 embeddings and city2 embeddings
        distances = euclidean_distances([neighborhood1_combined_embedding], city2_combined_embeddings)

        # find and return closest neighborhood in city2
        closest_idx = np.argmin(distances)
        return city2_in_same_cluster.iloc[closest_idx]["Neighborhood"]
    else:
        print("No neighborhoods found in the same cluster as neighborhood1 in city2.")
        # use KNN without clusters if no neighborhoods from city2 in the same cluster as neighborhood1
        city2_text = city2_data.iloc[:, 4:]
        distances = euclidean_distances(neighborhood1_data, city2_text)

        # find and return closest neighborhood in city2
        closest_idx = np.argmin(distances)
        return city2_data.iloc[closest_idx]["Neighborhood"]

### Testing
closest_w_cluster = find_closest_neighborhood_cluster("New York City", "Bushwick and Williamsburg", "San Francisco", df, optimal_k)
print("Closest neighborhood:", closest_w_cluster)
